{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistemas Inteligentes: Aprendizaje Automático\n",
    "\n",
    "_Guillermo Facundo Colunga_\n",
    "\n",
    "**Datos entorno desarrollo:**  \n",
    "**SO:** Ubuntu Server, virtualizado en VirtualBox.  \n",
    "**Cores:** 32.  \n",
    "**RAM:** 30.7 Gb.  \n",
    "**Librerías empleadas:** `caret`, `ggplot2`, `gridExtra`, `reshape2` y `doMC`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "Esta práctica de aprendizaje automático se enmarca en explorar los distintos paradigmas de clasificación estudiados en la asignatura para seleccionar aquel que mejor prediga el tipo de cancer según la información de expresión de los genes de distintas muestras de pacientes reales.\n",
    "\n",
    "Para ello se evaluarán los paradigmas de clasificación **árbol de decisión, vecinos más cercanos, redes neuronales y máquinas de vector soporte** a través del paquete `caret` de R. \n",
    "\n",
    "Finalmente se realizará la comparación de los resultados aportados por los diferentes paradigmas y se seleccionará aquel que mejor precisión tenga como _el mejor_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos configurando R e importanto el paquete `caret`. De la misma forma cargamos las librerías necesarias que vamos a utilizar durante el desarrollo de la práctica y eliminamos los warnings de nuestro script para limpiar las salidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n",
      "\n",
      "Loading required package: ggplot2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos los warnings del script ya que ensucian el script\n",
    "options(warn=-1)\n",
    "\n",
    "# Realizamos los imports necesarios\n",
    "library(caret)\n",
    "#library(mlbench)\n",
    "library(ggplot2)\n",
    "library(gridExtra)\n",
    "library(reshape2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos en R\n",
    "\n",
    "Una vez tenemos R configurado y con las librerías necesarias cargadas, importamos los datos sobre los que vamos a dearrollar nuestros modelos. Para ello descargamos en el directorio `/data` el siguiente banco de [datos](https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq) y lo importamos con el siguiente código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in file(file, \"rt\"): no se puede abrir la conexión\n",
     "output_type": "error",
     "traceback": [
      "Error in file(file, \"rt\"): no se puede abrir la conexión\nTraceback:\n",
      "1. read.csv(\"./data/data.csv\", stringsAsFactors = F)",
      "2. read.table(file = file, header = header, sep = sep, quote = quote, \n .     dec = dec, fill = fill, comment.char = comment.char, ...)",
      "3. file(file, \"rt\")"
     ]
    }
   ],
   "source": [
    "geneLevel <- read.csv('./data/data.csv',stringsAsFactors = F)\n",
    "label <- read.csv('./data/labels.csv',stringsAsFactors = F)\n",
    "tcga <- merge(geneLevel, label, sort = F)\n",
    "tcga$X <- NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El banco de datos `tcga` contiene 20531 variables y 801 instancias. Para manejar tal cantidad de columnas en algunos casos haría falta una cantidad de memoria superior a la de un ordenador típico. Por esto solamente vamos a usar 100 variables que serán elegidas aletaroiamente según nuestro identificador universitario. De esta forma obtendremos un banco de datos de 100 variables y 801 instancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuramos nuestro identificador universitario\n",
    "uo <- 236856\n",
    "\n",
    "set.seed(uo)\n",
    "tcga.filtered <- tcga[,c(sample(ncol(tcga)-1,100),ncol(tcga))]\n",
    "\n",
    "# Mostramos los datos con los que trabajaremos.\n",
    "head(tcga.filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos en la tabla anterior cada columna se corresponde con la información de expresión de un gen y la última columna identifica el tipo de cáncer. Así mismo en la tabla anterior se puede ver que hay dos genes en particular que presentan valores de 0 para las entradas mostradas, en el caso de que fueran 0 en todas y cada una de las instancias podrían eliminarse, sin embargo para esto debemos de ver el sumario de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary(tcga.filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la tabla anterior podemos ver que todas las variables pueden ser relevantes para el aprendizaje y por tanto no realizaremos ninún otro pre-proceso, a parte del de filtrado, de datos.\n",
    "\n",
    "Observando los datos en formato de tabla es muy complicado sacar cualquier conclusión preliminar sobre nuestros datos, por eso los exploraremos por médio de gráficas (para mejorar la resolución de las gráficas seleccionaremos únicamente los últimos 4 genes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamaño de las gráficas.\n",
    "options(repr.plot.width=10, repr.plot.height=5)\n",
    "\n",
    "ggplot(melt(tcga.filtered[(length(tcga.filtered) - 4):length(tcga.filtered)]),\n",
    "       aes(x=value,color=Class, fill = Class)) + \n",
    "       geom_density(adjust = 1/2, alpha=0.1) +\n",
    "       ggtitle(\"Distribución de valores según la clase\") +\n",
    "       theme(plot.title = element_text(hjust = 0.5)) + facet_grid(rows = Class~variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la gráfica anterior vemos los valores de las variables de forma individuales, lo que nos puede dar una idea inicial del tipo de probelma que es y que modelo se ajustaría mejor, sin embargo lo que nos interesará será ver como se relacionan las variables entre ellas, ya que un modelo predictivo intenta combinar las variables y sus interacciones.\n",
    "\n",
    "Para ello vamos a ver la relación entre la información de expresión de los 2 últimos genes y el tipo de cáncer al que pertenencen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(tcga.filtered, aes(x=gene_8981,y=gene_17533,color=Class)) + \n",
    "    geom_point() + \n",
    "    ggtitle(\"Diagrama de disperesión para las variables gene_8981 y gene_17533\") +\n",
    "    theme(plot.title = element_text(hjust = 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríamos realizar la gráfica anterior 16 veces (1 para cada par de genes), sin embargo para verlo todo en un mismo bloque emplearemos una variante de la función makePairs vista en práctica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makePairs <- function(data,classIndex) { \n",
    "    dfm <- NULL\n",
    "    for(i in 1:ncol(data)){\n",
    "        if(i==classIndex)\n",
    "          next\n",
    "        for(j in 1:ncol(data)){ \n",
    "            if(j==classIndex)\n",
    "                next\n",
    "            if(i==j)\n",
    "                next\n",
    "            dfm <- rbind(dfm,data.frame(x=data[,i], y=data[,j], \n",
    "                                        Var1=colnames(data)[i], \n",
    "                                        Var2=colnames(data)[j],\n",
    "                                        class=data[,classIndex]))\n",
    "        }\n",
    "    }\n",
    "    dfm$Var2 <- relevel(dfm$Var2,levels(dfm$Var1)[1])\n",
    "    dfm \n",
    "}\n",
    "            \n",
    "dataToPlot <- makePairs(tcga.filtered[(length(tcga.filtered) - 4):length(tcga.filtered)], 5)\n",
    "ggplot(dataToPlot, aes_string(x = \"x\", y = \"y\")) + \n",
    "            facet_grid(Var1 ~ Var2, scales = \"free\") + \n",
    "            geom_point(aes(colour=class), alpha=0.6) + \n",
    "            ggtitle(\"Diagrama de dispersión para cada clase\") +\n",
    "            theme(plot.title = element_text(hjust = 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidentemente las conclusiones están limitadas por que sólo vemos relaciones en dos dimensiones, pero aún con esta limitación realizar esta tarea es importante para entender peculiaridades de los datos, detectar posibles problemas y contrastar lo que hemos podido inferir con lo que un modelo dice.\n",
    "\n",
    "Una vez hemos entendido los datos sobre los que vamos a trabajar tenemos que realizar 2 particiones, una con la que entrenaremos nuestros modelos y otra con la que los validaremos. En este caso yaque tenemos 801 instancias y sólo 100 variables escogeremos un 75% de los datos para entrenamiento y un 25% para la validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inTrain <- createDataPartition(\n",
    "    # indicamos la clase\n",
    "    y = tcga.filtered$Class,\n",
    "    \n",
    "    # el porcentaje de instancias para entrenamiento\n",
    "    p = .75,\n",
    "    \n",
    "    # controla el formato de salida\n",
    "    list = FALSE\n",
    ")\n",
    "\n",
    "# Definimos nuestras particiones de entrenamiento y validación.\n",
    "training <- tcga.filtered[ inTrain,]\n",
    "testing <- tcga.filtered[-inTrain,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esquemas de validación\n",
    "\n",
    "Conocemos que existen distintos tipos de validación, cada uno con sus ventajas y desventajas. Sin embargo, para nuestro problema donde tenemos 801 instancias y 100 variables el que más adecuado a emplear será el de validación cruzada con k 10 (número de subconjuntos a usar en la validación). Esto viene dado de que a priori, no hay un único validador que sea el mejor para todos y cada uno de los modelos, sin embargo tras realizar una comparación entre ellos es el de validación cruzada el que nos ofrece una mejor porcentaje de acierto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossValidation <- trainControl(method = \"cv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árbol de decisión\n",
    "\n",
    "De acuerdo con la teoría, un árbol de decisión es un mecanismo en el que dado un conjunto de datos se fabrican diagramas de construcciones lógicas, muy similares a los sistemas de predicción basados en reglas, que sirven para representar y categorizar una serie de condiciones que ocurren de forma sucesiva, para la resolución de un problema. Por lo tanto, y teniendo en cuenta nuestros datos, parece un modelo que nos dará unos buenos resultados, sin embargo, vamos a contrastar esta hipótesis.\n",
    "\n",
    "Tanto en teroría como en prácticas vimos distintos modelos de árboles de decisión, a continuación implementaremos los modelos J48, rpart y rpart2 para comparar su precisión para el problema dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantamos la semilla para obtener un valor constante tras cada ejecución.\n",
    "set.seed(uo)\n",
    "\n",
    "# Realizamos el entrenamiento de nuestro modelo con J48.\n",
    "c4.5Fit <- train(\n",
    "    Class ~ .,\n",
    "    data = training,\n",
    "    method = \"J48\",\n",
    "    trControl = crossValidation,\n",
    "    # Empleamos C=0.04 y M=2 ya que son los valores que tras explorar\n",
    "    # nos han dado mejor resultado y realizar siempre la exploración\n",
    "    # de valores es demasiado lenta.\n",
    "    tuneGrid = data.frame(C=0.04, M=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A partir de este punto podemos emplear multriproceso para los entrenamientos. Para aprovechar al máximo los recursos del ordenador emplearemos el paquete `multicore`. Debemos de tener en cuenta que actualmente R sólo da soporte a múltiples núcleos en Linux, por lo tanto si se ejecuta desde otro S.O. es de esperar que no funcione, en ese caso se pueden eliminar las siguientes lineas y todo funcionará sin problemas en un solo core.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(doMC)\n",
    "\n",
    "# Leave always one core free for other processes.\n",
    "nCores <- detectCores(all.tests = FALSE, logical = TRUE) -1\n",
    "registerDoMC(cores = nCores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantamos la semilla para obtener un valor constante tras cada ejecución.\n",
    "set.seed(uo)\n",
    "\n",
    "# Realizamos el entrenamiento de nuestro modelo con rpart.\n",
    "rpartFit <- train(\n",
    "    Class ~ .,\n",
    "    data = training,\n",
    "    method = \"rpart\",\n",
    "    trControl = crossValidation,\n",
    "    # Exploramos los resultados para 200 valores de cp crecientes desde 0.0.\n",
    "    tuneLength = 200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantamos la semilla para obtener un valor constante tras cada ejecución.\n",
    "set.seed(uo)\n",
    "\n",
    "# Realizamos el entrenamiento de nuestro modelo con rpart2.\n",
    "rpart2Fit <- train(\n",
    "    Class ~ .,\n",
    "    data = training,\n",
    "    method = \"rpart2\",\n",
    "    trControl = crossValidation,\n",
    "    # Exploramos los resultados hasta una profundidad de 8, que es la\n",
    "    # profundidad máxima del arbol.\n",
    "    tuneLength = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comparar los modelos de manera inicial emplearemos el sistema de matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos del modelo rpart2 y matriz de confusión para los datos de validación\n",
    "c4.5Fit$results[rownames(c4.5Fit$bestTune),1:4]\n",
    "confusionMatrix(predict(c4.5Fit,testing), factor(testing$Class))$overall[1:4]\n",
    "\n",
    "# Datos del modelo rpart2 y matriz de confusión para los datos de validación\n",
    "rpartFit$results[rownames(rpartFit$bestTune),1:4]\n",
    "confusionMatrix(predict(rpartFit,testing), factor(testing$Class))$overall[1:4]\n",
    "\n",
    "# Datos del modelo rpart2 y matriz de confusión para los datos de validación\n",
    "rpart2Fit$results[rownames(rpart2Fit$bestTune),1:4]\n",
    "confusionMatrix(predict(rpart2Fit,testing), factor(testing$Class))$overall[1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De los modelos datos anteriores podemos extraer direfentes conclusiones preliminares. Por ejemplo, tanto rpart como rpart2 obtiene una precisión mayor sobre los datos de entrenamiento que j48, sin embargo cuando realizamos la validación externa se puede observar que es precisamente j48 el que obtiene un resultado superior a rpart y rpart2. \n",
    "\n",
    "Sin embargo antes de realizar la conclusión de que j48 es el modelo más preciso debemos de realizar algunas comparaciones para ver si la mejora es significativa, o por la contra los resultados de los modelos son equivalentes. En este caso como rpart y rpart2 obtienen exactamente los mismos resultados tanto para datos de entrenamiento como para datos de validación realizar la comparación entre c4.5 y rpart será suficiente para extraer conclusiones de los 3 modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test binomial entre c4.5(j48) y rpart \n",
    "binom.test(c(round(confusionMatrix(predict(c4.5Fit,testing), factor(testing$Class))$overall[1]),\n",
    "             round(1-confusionMatrix(predict(c4.5Fit,testing), factor(testing$Class))$overall[1])\n",
    "            )*nrow(testing),\n",
    "           p = confusionMatrix(predict(rpartFit,testing), factor(testing$Class))$overall[1])\n",
    "\n",
    "# Test entre c4.5 y rpart de validación cruzada.\n",
    "t.test(c4.5Fit$resample$Accuracy,rpartFit$resample$Accuracy,paired = T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El test binomial nos indica que la diferencia en precisión entre c4.5 (j48), rpart y rpart2 es significativa y por lo tanto podemos decir que c4.5 obtiene una precisión significativamente mayor.\n",
    "\n",
    "El segundo test nos indica que durante el entrenamiento no se puede tomar la diferencia\n",
    "como significativa. Los dos modelos ajustan los datos de entrenamiento de manera similar pero el modelo c4.5 es capaz de generalizar mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vecinos más cercanos\n",
    "\n",
    "Otro de los paradigmas de clasificación vistos durante la asginatura es el de vecinos más cercanos o knn por sus siglas en inglés. Este paradigma no tiene un modelo, lo que hace es comparar el ejemplo con los datos a predecir y a través de distancias a k vecinos más cercanos precedir la clase.\n",
    "\n",
    "Para iniciar nuestro modelo de vecinos más cercanos emplearemos 100 valores de k dados por la variable `tuneLength`. En principio tenemos 801 instancias del problema así que una k de 100 representaría 1/8 del mismo, sin embargo como veremos a continuación el valor ideal de k será mucho más bajo, ya que no es necesario emplear tantos puntos para predecir nuestro resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantamos la semilla para obtener un valor constante tras cada ejecución.\n",
    "set.seed(uo)\n",
    "\n",
    "# Realizamos el entrenamiento de nuestro modelo con knn.\n",
    "knnFit <- train(\n",
    "  Class ~ .,\n",
    "  data = training,\n",
    "  method = \"knn\",\n",
    "  trControl = crossValidation,\n",
    "  tuneLength = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnFit$results[rownames(knnFit$bestTune),1:4]\n",
    "confusionMatrix(predict(knnFit,testing), factor(testing$Class))$overall[1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se explicaba previamente y con los resultados anteriores el valor de k que nos proporciona una precisión mayor sobre los datos de entrenamiento es 7, mucho más pequeña que la inicialmente dada, 100. Esto nos dice que comparando los datos a predecir con 7 vecinos más cercanos obtenemos una precisión de 96.98% sobre los datos de validación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neuronales\n",
    "\n",
    "Este es un método de clasificación no paramétrico, que estima el valor de la función de densidad de probabilidad o directamente la probabilidad a posteriori de que un elemento x pertenezca a la clase C a partir de la información proporcionada por el conjunto de prototipos. En el proceso de aprendizaje no se hace ninguna suposición acerca de la distribución de las variables predictoras.\n",
    "\n",
    "Ahora pasamos a crear un primer modelo de red neuronal con una sola capa oculta. Para determinar el número de unidades ocultas pedimos a la función train que pruebe varios valores de 5 (número de clases) hasta 68 (número de entradas x 0.66 + 2 ) incrmenetando cada vez una unidad. Para estas redes se ha fijado el ratio de aprendizaje en 0.02 ya que es el que mejor resultados a aportado de entre los probados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantamos la semilla para obtener un valor constante tras cada ejecución.\n",
    "set.seed(uo)\n",
    "\n",
    "# Realizamos el entrenamiento de nuestro modelo\n",
    "nn1LFit <- train(\n",
    "    Class ~ .,\n",
    "    data = training,\n",
    "    method = \"mlp\",\n",
    "    trControl = crossValidation,\n",
    "    learnFuncParams = c(0.02,0),\n",
    "    tuneGrid = data.frame(size=seq(5,68,1))\n",
    ")\n",
    "\n",
    "nneurons1L <- nn1LFit$results[rownames(nn1LFit$bestTune),1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez entrenado nuestro modelo podemos ver los siguientes resultados respecto a los datos de entrenamiento y los datos de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de una única capa oculta.\n",
    "nn1LFit$results[rownames(nn1LFit$bestTune),1:4]\n",
    "\n",
    "# Matriz de confusión para el modelo de una única capa oculta.\n",
    "confusionMatrix(predict(nn1LFit,testing), factor(testing$Class))$overall[1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el modelo anterior encontramos que la precisión más elevada se da cuando empleamos 60 neuronas en la capa oculta, con un 98.8% sobre los datos de entrenamiento y un 98.99% sobre los datos de validación.\n",
    "\n",
    "Vamos a probar con redes más profundas, pero limitando la complejidad (número de parámetros) a un nivel similar a la red anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantamos la semilla para obtener un valor constante tras cada ejecución.\n",
    "set.seed(uo)\n",
    "\n",
    "# Realizamos el entrenamiento de nuestro modelo\n",
    "nn2LFit <- train(\n",
    "    Class ~ .,\n",
    "    data = training,\n",
    "    method = \"mlpML\",\n",
    "    trControl = crossValidation,\n",
    "    learnFuncParams = c(0.02,0),\n",
    "    tuneGrid = expand.grid(layer1=seq(nneurons1L-5,nneurons1L+6,1),\n",
    "                           layer2=seq(nneurons1L-5,nneurons1L+6,1),\n",
    "                           layer3=0)\n",
    ")\n",
    "\n",
    "nneurons2L1 <- nn2LFit$results[rownames(nn2LFit$bestTune),1]\n",
    "nneurons2L2 <- nn2LFit$results[rownames(nn2LFit$bestTune),2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez entrenado nuestro modelo podemos ver los siguientes resultados respecto a los datos de entrenamiento y los datos de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de 2 capas ocultas.\n",
    "nn2LFit$results[rownames(nn2LFit$bestTune),1:4]\n",
    "\n",
    "# Matriz de confusión para el modelo de dos capas ocultas.\n",
    "confusionMatrix(predict(nn2LFit,testing), factor(testing$Class))$overall[1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los resultados anteriores podemos observar que para este problema una red neuronal de dos capas da una precisión ligeramente inferior a la red de una capa. Sin embargo vamos a explorar también el modelo construido con 3 capas ocultas para ver si una red aún más profunda favorece a la precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantamos la semilla para obtener un valor constante tras cada ejecución.\n",
    "set.seed(uo)\n",
    "\n",
    "# Realizamos el entrenamiento de nuestro modelo\n",
    "nn3LFit <- train(\n",
    "    Class ~ .,\n",
    "    data = training,\n",
    "    method = \"mlpML\",\n",
    "    trControl = crossValidation,\n",
    "    learnFuncParams = c(0.02,0),\n",
    "    tuneGrid = expand.grid(layer1=seq(nneurons1L-1,nneurons1L+2,1),\n",
    "                           layer2=seq(nneurons1L-1,nneurons1L+2,1),\n",
    "                           layer3=seq(nneurons1L-1,nneurons1L+2,1))\n",
    ")\n",
    "\n",
    "nneurons3L1 <- nn3LFit$results[rownames(nn3LFit$bestTune),1]\n",
    "nneurons3L2 <- nn3LFit$results[rownames(nn3LFit$bestTune),2]\n",
    "nneurons3L3 <- nn3LFit$results[rownames(nn3LFit$bestTune),3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez entrenado nuestro modelo podemos ver los siguientes resultados respecto a los datos de entrenamiento y los datos de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Modelo de tres capas ocultas.\n",
    "nn3LFit$results[rownames(nn3LFit$bestTune),1:4]\n",
    "\n",
    "# Matriz de confusión para el modelo de tres capas ocultas.\n",
    "confusionMatrix(predict(nn3LFit,testing), factor(testing$Class))$overall[1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que el modelo de 3 capas ocultas reduce la precisión sobre los datos de entrenamiento y de validación, por lo tanto, en una primera hipótesis el modelo de una capa oculta es el mejor para nuestro problema.\n",
    "\n",
    "Para intentar entender estas diferencias vamos a inspeccionar como ha ido el proceso de aprendizaje de cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del tamaño de las gráficas.\n",
    "options(repr.plot.width=10, repr.plot.height=3)\n",
    "\n",
    "errorNn1Err <- ggplot() + geom_line(aes(x=1:length(nn1LFit$finalModel$IterativeFitError), y=nn1LFit$finalModel$IterativeFitError)) +\n",
    "xlab(\"Iteraciones\") + ylab(\"Error\") + ggtitle(\"1 HLayer Model\") + theme(plot.title = element_text(hjust = 0.5))\n",
    "\n",
    "errorNn2Err <- ggplot() + geom_line(aes(x=1:length(nn2LFit$finalModel$IterativeFitError), y=nn2LFit$finalModel$IterativeFitError)) +\n",
    "xlab(\"Iteraciones\") + ylab(\"Error\") + ggtitle(\"2 HLayer Model\") + theme(plot.title = element_text(hjust = 0.5))\n",
    "\n",
    "errorNn3Err <- ggplot() + geom_line(aes(x=1:length(nn3LFit$finalModel$IterativeFitError), y=nn3LFit$finalModel$IterativeFitError)) +\n",
    "xlab(\"Iteraciones\") + ylab(\"Error\") + ggtitle(\"3 HLayer Model\") + theme(plot.title = element_text(hjust = 0.5))\n",
    "\n",
    "grid.arrange(errorNn1Err, errorNn2Err, errorNn3Err, ncol=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De las curvas de aprendizaje anteriores podemos ver que en ninguna de ellas el error cometido alcanza un valor estable y está decreciendo en el momento de terminar las iteraciones (100 por defecto).\n",
    "\n",
    "Pues ahora vamos a generar un modelo fijando el número de neuronas que daba el mejor resultado, pero aumentando el número de iteraciones. Previamente se ha comprovado que para valores mayores de 1000 el error permanece más o menos constante en todos los modelos, por tanto el valor `maxit` se corresponderá con el que mejor resultado de para los datos de validación. Así mismo también fijaremos el ratio de aprendizaje en 0.03 ya que es el que mejores resultados nos da."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantamos la semilla para obtener un valor constante tras cada ejecución.\n",
    "set.seed(uo)\n",
    "\n",
    "# Realizamos el entrenamiento de nuestro modelo\n",
    "nn1LFitOptimized <- train(\n",
    "    Class ~ .,\n",
    "    data = training,\n",
    "    method = \"mlp\",\n",
    "    trControl = crossValidation,\n",
    "    tuneGrid = data.frame(size=nneurons1L),\n",
    "    maxit = 500,\n",
    "    learnFuncParams = c(0.02,0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantamos la semilla para obtener un valor constante tras cada ejecución.\n",
    "set.seed(uo)\n",
    "\n",
    "# Realizamos el entrenamiento de nuestro modelo\n",
    "nn2LFitOptimized <- train(\n",
    "    Class ~ .,\n",
    "    data = training,\n",
    "    method = \"mlpML\",\n",
    "    trControl = crossValidation,\n",
    "    tuneGrid = expand.grid(layer1=nneurons2L1, layer2=nneurons2L2, layer3=0),\n",
    "    maxit = 1500,\n",
    "    learnFuncParams = c(0.005,0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantamos la semilla para obtener un valor constante tras cada ejecución.\n",
    "set.seed(uo)\n",
    "\n",
    "# Realizamos el entrenamiento de nuestro modelo\n",
    "nn3LFitOptimized <- train(\n",
    "    Class ~ .,\n",
    "    data = training,\n",
    "    method = \"mlpML\",\n",
    "    trControl = crossValidation,\n",
    "    tuneGrid = expand.grid(layer1=nneurons3L1, layer2=nneurons3L2, layer3=nneurons3L3),\n",
    "    maxit = 1750,\n",
    "    learnFuncParams = c(0.005,0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez realizado el entrenamiento de los tres modelos con el número de iteraciones fijado en 1000, 1000 y 1500 podemos volver a obtener los resultados con respecto a los datos de entrenamiento y validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de una única capa oculta.\n",
    "nn1LFitOptimized$results[rownames(nn1LFitOptimized$bestTune),1:4]\n",
    "\n",
    "# Matriz de confusión para el modelo de una única capa oculta.\n",
    "confusionMatrix(predict(nn1LFitOptimized,testing), factor(testing$Class))$overall[1:4]\n",
    "\n",
    "# Modelo de 2 capas oculta con 5-7 neuronas.\n",
    "nn2LFitOptimized$results[rownames(nn2LFitOptimized$bestTune),1:4]\n",
    "\n",
    "# Matriz de confusión para el modelo de una única capa oculta y 7 neuronas.\n",
    "confusionMatrix(predict(nn2LFitOptimized,testing), factor(testing$Class))$overall[1:4]\n",
    "\n",
    "# Modelo de tres capas ocultas con 5-5-5 neuronas.\n",
    "nn3LFitOptimized$results[rownames(nn3LFitOptimized$bestTune),1:4]\n",
    "\n",
    "# Matriz de confusión para el modelo de una única capa oculta y 7 neuronas.\n",
    "confusionMatrix(predict(nn3LFitOptimized,testing), factor(testing$Class))$overall[1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los resultados anteriores podemos observar que el modelo de una capa oculta es aquel que mejor precisión nos da sobre los datos de aprendizaje. Pero antes de realizar cualquier conclusión vamos a ver las curvas de apredizaje y realizar los tests binomiales y de t para ver si las diferencias son significativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Configuración del tamaño de las gráficas.\n",
    "options(repr.plot.width=10, repr.plot.height=3)\n",
    "\n",
    "errorNn1OptimizedErr <- ggplot() + geom_line(aes(x=1:length(nn1LFitOptimized$finalModel$IterativeFitError), y=nn1LFitOptimized$finalModel$IterativeFitError)) +\n",
    "xlab(\"Iteraciones\") + ylab(\"Error\") + ggtitle(\"1 HLayer Model\") + theme(plot.title = element_text(hjust = 0.5))\n",
    "\n",
    "errorNn2OptimizedErr <- ggplot() + geom_line(aes(x=1:length(nn2LFitOptimized$finalModel$IterativeFitError), y=nn2LFitOptimized$finalModel$IterativeFitError)) +\n",
    "xlab(\"Iteraciones\") + ylab(\"Error\") + ggtitle(\"2 HLayer Model\") + theme(plot.title = element_text(hjust = 0.5))\n",
    "\n",
    "errorNn3OptimizedErr <- ggplot() + geom_line(aes(x=1:length(nn3LFitOptimized$finalModel$IterativeFitError), y=nn3LFitOptimized$finalModel$IterativeFitError)) +\n",
    "xlab(\"Iteraciones\") + ylab(\"Error\") + ggtitle(\"3 HLayer Model\") + theme(plot.title = element_text(hjust = 0.5))\n",
    "\n",
    "grid.arrange(errorNn1OptimizedErr, errorNn2OptimizedErr, errorNn3OptimizedErr, ncol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test binomial entre nn1LFitOptimized y nn2LFitOptimized \n",
    "binom.test(c(round(confusionMatrix(predict(nn1LFitOptimized,testing), factor(testing$Class))$overall[1]),\n",
    "             round(1 - confusionMatrix(predict(nn1LFitOptimized,testing), factor(testing$Class))$overall[1])\n",
    "            )*nrow(testing),\n",
    "           p = confusionMatrix(predict(nn2LFitOptimized,testing), factor(testing$Class))$overall[1])\n",
    "\n",
    "# Test entre n1LFitOptimized y nn2LFitOptimized de validación cruzada.\n",
    "t.test(nn1LFitOptimized$resample$Accuracy, nn2LFitOptimized$resample$Accuracy,paired = T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El test binomial nos indica que durante el entrenamiento no se puede tomar la diferencia como significativa. Los dos modelos ajustan los datos de entrenamiento de manera similar pero el modelo de una única capa oculta es capaz de generalizar mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Máquinas vector soporte\n",
    "\n",
    "Estos métodos están propiamente relacionados con problemas de clasificación y regresión. Dado un conjunto de ejemplos de entrenamiento (de muestras) podemos etiquetar las clases y entrenar una SVM para construir un modelo que prediga la clase de una nueva muestra. Intuitivamente, una SVM es un modelo que representa a los puntos de muestra en el espacio, separando las clases a 2 espacios lo más amplios posibles mediante un hiperplano de separación definido como el vector entre los 2 puntos, de las 2 clases, más cercanos al que se llama vector soporte. Cuando las nuevas muestras se ponen en correspondencia con dicho modelo, en función de los espacios a los que pertenezcan, pueden ser clasificadas a una o la otra clase.\n",
    "\n",
    "De la misma forma que con los árboles de decisión para las máquinas de vector soporte encontramos diferentes tipos, Lineares, Polinomial y Radial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantamos la semilla para obtener un valor constante tras cada ejecución.\n",
    "set.seed(uo)\n",
    "\n",
    "svmLinealFit <- train(\n",
    "    Class ~ .,\n",
    "    data = training,\n",
    "    method = \"svmLinear\",\n",
    "    trControl = crossValidation,\n",
    "    tuneGrid = data.frame(C=1),\n",
    "    scale = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantamos la semilla para obtener un valor constante tras cada ejecución.\n",
    "set.seed(uo)\n",
    "\n",
    "svmPolyFit <- train(\n",
    "    Class ~ .,\n",
    "    data = training,\n",
    "    method = \"svmPoly\",\n",
    "    trControl = crossValidation,\n",
    "    tuneGrid = expand.grid(degree=c(2,3),C=c(1,5),scale = c(0.1, 0.5, 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantamos la semilla para obtener un valor constante tras cada ejecución.\n",
    "set.seed(uo)\n",
    "\n",
    "svmRadialFit <- train(\n",
    "    Class ~ .,\n",
    "    data = training,\n",
    "    method = \"svmRadial\",\n",
    "    trControl = crossValidation, \n",
    "    tuneGrid = expand.grid(sigma=c(1,3),C=c(1,5)) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los modelos anteriores entrenamos podemos mostrar las precisiones respecto a los datos de entrenamiento y a los datos de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmLinealFit$results[rownames(svmLinealFit$bestTune),1:4]\n",
    "confusionMatrix(predict(svmLinealFit,testing),factor(testing$Class))$overall[1:2]\n",
    "\n",
    "svmPolyFit$results[rownames(svmPolyFit$bestTune),1:5]\n",
    "confusionMatrix(predict(svmPolyFit,testing),factor(testing$Class))$overall[1:2]\n",
    "\n",
    "svmRadialFit$results[rownames(svmRadialFit$bestTune),1:4]\n",
    "confusionMatrix(predict(svmRadialFit,testing),factor(testing$Class))$overall[1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para los resultados aportados por los modelos anteriores no es necesario realizar ningún test de validación ya que a simple vista se puede ver que el modelo lineal tiene una precisión muy superior al resto de modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de los paradigmas\n",
    "\n",
    "Para realizar la comparación de los paradigmas explorados seleccionaremos sólo aquellos modelos que durante el desarrollo de la práctica identificamos como los _mejores_. Estos fueron: **C4.5** para árboles de decisión, **knn** para vecinos más cercanos, el modelo de **una única capa oculta optimizado en iteraciones** para las redes neuronales y finalmente el **modelo lineal** para las máquinas de vector soporte. Los resultados obtenidos para la validación externa de cada paradigma són los siguientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c4.5Fit.accuracy <- confusionMatrix(predict(c4.5Fit,testing), factor(testing$Class))$overall[1:1]\n",
    "cat(\"C4.5 -\", c4.5Fit.accuracy, \"\\n\")\n",
    "\n",
    "knnFit.accuracy <- confusionMatrix(predict(knnFit,testing), factor(testing$Class))$overall[1:1]\n",
    "cat(\"knn -\", knnFit.accuracy, \"\\n\")\n",
    "\n",
    "nn1LFitOptimized.accuracy <- confusionMatrix(predict(nn1LFitOptimized,testing), factor(testing$Class))$overall[1:1]\n",
    "cat(\"nn1L -\", nn1LFitOptimized.accuracy, \"\\n\")\n",
    "\n",
    "svmLinealFit.accuracy <- confusionMatrix(predict(svmLinealFit,testing), factor(testing$Class))$overall[1:1]\n",
    "cat(\"svmLineal -\", svmLinealFit.accuracy, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora realizaremos las comparaciones con el test binomial entre el modelo que parece más preciso, nn1L y el resto de modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test binomial entre knn y c4.5 \n",
    "binom.test(c(round(nn1LFitOptimized.accuracy), round(1-nn1LFitOptimized.accuracy))*nrow(testing), p = c4.5Fit.accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El test binomial entre el modelo nn1L y el modelo c4.5 nos dice que nn1L es significativamente más preciso para los datos de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test binomial entre knn y nn1L \n",
    "binom.test(c(round(nn1LFitOptimized.accuracy), round(1-nn1LFitOptimized.accuracy))*nrow(testing), p = knnFit.accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El test binomial entre el modelo nn1L y el modelo knn nos dice que nn1L es significativamente más preciso para los datos de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test binomial entre knn y svmLinealFit \n",
    "binom.test(c(round(nn1LFitOptimized.accuracy), round(1-nn1LFitOptimized.accuracy))*nrow(testing), p = svmLinealFit.accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El test binomial entre el modelo nn1L y el modelo svmLineal nos dice que nn1L es significativamente más preciso para los datos de validación.\n",
    "\n",
    "Ahora observemos una gráfica de comparación con los modelos más precisos de cada paradigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=10, repr.plot.height=5)\n",
    "dfm.resamp <- data.frame(Accuracy = c(nn1LFitOptimized$resample$Accuracy,\n",
    "                                      c4.5Fit$resample$Accuracy,\n",
    "                                      knnFit$resample$Accuracy,\n",
    "                                      svmLinealFit$resample$Accuracy), \n",
    "                        Model = rep(c('NeuralNet','C4.5','kNN', 'MVS'), each=10))\n",
    "ggplot(dfm.resamp,aes(x=Model, y=Accuracy, fill=Model)) + geom_boxplot() +\n",
    "ggtitle(\"Distribución del porcetaje de acierto en el proceso de aprendizaje para cada paradigma\") +\n",
    "theme(plot.title = element_text(hjust = 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De las comparaciones anteriores y la gráfica final se puede deducir que el modelo de red neuronal con una única capa oculta es aquel que presenta una mayor precisión sobre los datos de validación. Así mismo, comparado con los modelos knn y máquinas vector soporte, los modelos que tiene una precisión similar, es el modelo de redes neuronales el que es más estable. Sin embargo de entre todos los modelos el más estable es el de c4.5, que también es el que menos precisión nos da."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
